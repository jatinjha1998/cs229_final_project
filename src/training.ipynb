{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import initializations\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import trading as trd\n",
    "from trading import State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'trading' from 'C:\\\\Users\\\\hamza\\\\Documents\\\\school\\\\cs_229\\\\cs229_final_project\\\\src\\\\trading\\\\__init__.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# relaod module after modifying it\n",
    "import importlib\n",
    "importlib.reload(trd)\n",
    "importlib.reload(trd.stock_history)\n",
    "importlib.reload(trd.portfolio)\n",
    "importlib.reload(trd.benchmarks)\n",
    "importlib.reload(trd.environment)\n",
    "importlib.reload(trd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulate Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transaction cost to buy/sell a stock\n",
    "trans_cost = 0.001\n",
    "# starting cash\n",
    "cash = 1e4\n",
    "# starting portfolio allocation (%lo, %hi)\n",
    "starting_weights = (0.5, 0.5)\n",
    "# reward function (either Sharpe Ratio or last reward)\n",
    "reward = trd.sharpe_ratio_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of inputs\n",
    "n = State.num_states()\n",
    "# number of outputs\n",
    "k = trd.actions.size\n",
    "# size of training set\n",
    "m = 27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data, test_data = trd.get_stock_pairs(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "portfolio_states = [State(p, cash=cash, target_weights=starting_weights, trans_cost=trans_cost) for p in train_data]\n",
    "# list to delete from, keep all the portfolio states in portfolio_states\n",
    "#  generates a (shallow) copy rather than copy the list's reference \n",
    "available_states = portfolio_states[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# size of experience replay\n",
    "D = 3\n",
    "# alpha / learning rate\n",
    "α = 0.0001\n",
    "# discount factor\n",
    "γ = 0.99\n",
    "# ϵ-greedy parameter\n",
    "ϵ = 0.15\n",
    "# hidden layer size\n",
    "H = 100\n",
    "# activation function\n",
    "non_lin = 'sigmoid' #'relu'\n",
    "\n",
    "# custom init\n",
    "#  small starting seems to help\n",
    "scale = 1E-4\n",
    "def my_init(shape, name=None):\n",
    "    return initializations.normal(shape, scale=scale, name=name)\n",
    "#my_init = 'glorot_normal'\n",
    "\n",
    "# momentum in [0.5, 0.9, 0.95, 0.99]\n",
    "#opt = SGD(lr=α, decay=1e-5, momentum=0.95, nesterov=True)\n",
    "opt = Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(n=n, k=k, H=H, non_linearity=non_lin, init=my_init, opt=opt):\n",
    "    model = Sequential([\n",
    "        Dense(input_dim=n, output_dim=H, init=init),\n",
    "        Activation(non_linearity),\n",
    "        Dense(output_dim=H, init=init),\n",
    "        Activation(non_linearity),\n",
    "        Dense(output_dim=H, init=init),\n",
    "        Activation(non_linearity),\n",
    "        Dense(output_dim=k)])\n",
    "    \n",
    "    model.compile(loss='mean_squared_error', \n",
    "                  optimizer=opt, metrics=['mean_squared_error'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = create_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# target network drift\n",
    "τ = 0.001\n",
    "\n",
    "target = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def copy_model(target, model):\n",
    "    target.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def track_model(target, model, τ=τ):\n",
    "    model_weights = model.get_weights()\n",
    "    target_weights = target.get_weights()\n",
    "    new_weights = [τ * m + (1-τ)*t for (m, t) in zip(model_weights, target_weights)]\n",
    "    target.set_weights(new_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# start off with exact same init\n",
    "copy_model(target, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prints debugging info every so many iteration \n",
    "DEBUG = True\n",
    "DEBUG_EVERY = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iter:    1000\n",
      "reward: +0.0412908      \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'loss_value' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-eb651d712f7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\niter:   {:5d}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'reward: {:<+16g}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'loss:   {:<16g}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mi\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss_value' is not defined"
     ]
    }
   ],
   "source": [
    "train_record = pd.DataFrame(columns=('reward', 'loss'))\n",
    "i = 1\n",
    "\n",
    "#for _ in range(1):\n",
    "while True:\n",
    "    if available_states == []:\n",
    "        # nothing left :(\n",
    "        break\n",
    "    # fill the experience replay\n",
    "    elif len(available_states) < D:\n",
    "        exp_rep = np.random.permutation(available_states)\n",
    "    else:\n",
    "        exp_rep = np.random.choice(available_states, size=D, replace=False)\n",
    "\n",
    "    # the actual size of the experience replay\n",
    "    d = len(exp_rep)\n",
    "    \n",
    "    # actual state values of each portfolio\n",
    "    states = np.array([st.state for st in exp_rep])\n",
    " \n",
    "    qvalues = model.predict(states)\n",
    "\n",
    "    # max_a w/ ϵ\n",
    "    chosen_actions = trd.choose_actions(qvalues, ϵ)\n",
    "\n",
    "    for (st, a) in zip(exp_rep, trd.actions[chosen_actions]):\n",
    "        # execute the action\n",
    "        st.execute_trade(a)\n",
    "\n",
    "        # step forward to the next day\n",
    "        try:\n",
    "            st.step()\n",
    "        except StopIteration:\n",
    "            # reached end of data; no more stepping for this one\n",
    "            available_states.remove(st)\n",
    "\n",
    "    states_prime = np.array([st.state for st in exp_rep])\n",
    "    rewards = np.array([reward(st) for st in  exp_rep])\n",
    "\n",
    "    # max_a' Q(s', a')\n",
    "    # use target network \n",
    "    qvalues_prime = np.max(target.predict(states_prime), axis=1)\n",
    "\n",
    "    # the values we want (to minimize the MSE of)\n",
    "    qvalues[np.arange(0,d), chosen_actions] += rewards + γ * qvalues_prime\n",
    "    \n",
    "    # train the network\n",
    "    loss = model.train_on_batch(states, qvalues)\n",
    "    loss = np.asscalar(loss[-1])\n",
    "    \n",
    "    # allow the target to drift behind\n",
    "    track_model(target, model)\n",
    "    \n",
    "    if np.isnan(loss):\n",
    "        # we hit the rails . . .\n",
    "        # again\n",
    "        break\n",
    "\n",
    "    # append new value\n",
    "    # not very efficient, but this probably not the slowest step\n",
    "    train_record.loc[i,:] = [np.mean(rewards), loss]\n",
    "    \n",
    "    if (DEBUG) and (i % DEBUG_EVERY == 0):\n",
    "        print('\\niter:   {:5d}'.format(i))\n",
    "        print('reward: {:<+16g}'.format(np.mean(rewards)))\n",
    "        print('loss:   {:<16g}'.format(loss))\n",
    "        \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(i)\n",
    "print(loss_value)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
