{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import initializations\n",
    "from keras.optimizers import SGD, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import trading as trd\n",
    "from trading import State, create_model, copy_model, track_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'trading' from 'C:\\\\Users\\\\hamza\\\\Documents\\\\school\\\\cs_229\\\\cs229_final_project\\\\src\\\\trading\\\\__init__.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# relaod module after modifying it\n",
    "import importlib\n",
    "importlib.reload(trd)\n",
    "importlib.reload(trd.stock_history)\n",
    "importlib.reload(trd.portfolio)\n",
    "importlib.reload(trd.benchmarks)\n",
    "importlib.reload(trd.rl)\n",
    "importlib.reload(trd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulate Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transaction cost to buy/sell a stock\n",
    "trans_cost = 0.001\n",
    "# starting cash\n",
    "cash = 1e6\n",
    "# starting portfolio allocation (%lo, %hi)\n",
    "starting_weights = (0.5, 0.5)\n",
    "# reward function (either Sharpe Ratio or last reward)\n",
    "reward = trd.sharpe_ratio_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of inputs\n",
    "n = State.num_states()\n",
    "# number of outputs\n",
    "k = trd.actions.size\n",
    "# size of training set\n",
    "m = 27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data, test_data = trd.get_stock_pairs(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "portfolio_states = [State(p, cash=cash, target_weights=starting_weights, trans_cost=trans_cost) for p in train_data]\n",
    "# list to delete from, keep all the portfolio states in portfolio_states\n",
    "#  generates a (shallow) copy rather than copy the list's reference \n",
    "available_states = portfolio_states[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# size of experience replay\n",
    "D = 6\n",
    "# discount factor\n",
    "γ = 0.99\n",
    "# ϵ-greedy parameter\n",
    "ϵ = 0.15\n",
    "# hidden layer size\n",
    "H = 100\n",
    "# activation function\n",
    "non_lin = 'relu'\n",
    "\n",
    "# custom init\n",
    "#  small starting seems to help\n",
    "scale = 1E-4\n",
    "def my_init(shape, name=None):\n",
    "    return initializations.normal(shape, scale=scale, name=name)\n",
    "#my_init = 'glorot_normal'\n",
    "\n",
    "# alpha / learning rate\n",
    "#α = 0.0001\n",
    "# momentum in [0.5, 0.9, 0.95, 0.99]\n",
    "#opt = SGD(lr=α, decay=1e-5, momentum=0.95, nesterov=True)\n",
    "opt = Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = create_model(n=n, k=k, H=H, non_linearity=non_lin, init=my_init, optimizer=opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# target network drift\n",
    "τ = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target = create_model(n=n, k=k, H=H, non_linearity=non_lin, init=my_init, optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# start off with exact same init\n",
    "copy_model(target, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prints debugging info every so many iteration \n",
    "DEBUG = True\n",
    "DEBUG_EVERY = 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_record = pd.DataFrame(columns=('reward', 'loss'))\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iter:       0\n",
      "reward: +0              \n",
      "loss:   0.000464857     \n",
      "\n",
      "iter:    1000\n",
      "reward: +0.119442       \n",
      "loss:   0.00356383      \n",
      "\n",
      "iter:    2000\n",
      "reward: -0.0481931      \n",
      "loss:   0.00165883      \n",
      "\n",
      "iter:    3000\n",
      "reward: -0.0133415      \n",
      "loss:   0.00122855      \n",
      "\n",
      "iter:    4000\n",
      "reward: +0.0393694      \n",
      "loss:   0.000479116     \n",
      "\n",
      "iter:    5000\n",
      "reward: -0.00444697     \n",
      "loss:   0.000369338     \n",
      "\n",
      "iter:    6000\n",
      "reward: +0.0405035      \n",
      "loss:   0.000209102     \n",
      "\n",
      "iter:    7000\n",
      "reward: +0.0324255      \n",
      "loss:   7.01475e-05     \n",
      "\n",
      "iter:    8000\n",
      "reward: +0.0201244      \n",
      "loss:   0.000200227     \n",
      "\n",
      "iter:    9000\n",
      "reward: +0.0432288      \n",
      "loss:   0.000184521     \n",
      "\n",
      "iter:   10000\n",
      "reward: +0.0524292      \n",
      "loss:   9.21818e-05     \n",
      "\n",
      "iter:   11000\n",
      "reward: +0.0334165      \n",
      "loss:   0.000158889     \n",
      "\n",
      "iter:   12000\n",
      "reward: +0.0395492      \n",
      "loss:   2.6841e-05      \n",
      "\n",
      "iter:   13000\n",
      "reward: +0.0195314      \n",
      "loss:   2.8149e-05      \n",
      "\n",
      "iter:   14000\n",
      "reward: +0.0255102      \n",
      "loss:   0.000167486     \n",
      "\n",
      "iter:   15000\n",
      "reward: +0.0343382      \n",
      "loss:   0.000656381     \n",
      "\n",
      "iter:   16000\n",
      "reward: +0.00990612     \n",
      "loss:   0.000155011     \n",
      "\n",
      "iter:   17000\n",
      "reward: +0.00962453     \n",
      "loss:   3.97516e-05     \n",
      "\n",
      "iter:   18000\n",
      "reward: +0.0101139      \n",
      "loss:   2.19547e-05     \n",
      "\n",
      "iter:   19000\n",
      "reward: +0.0102884      \n",
      "loss:   2.27526e-05     \n",
      "\n",
      "iter:   20000\n",
      "reward: +0.0147215      \n",
      "loss:   3.58443e-05     \n",
      "\n",
      "iter:   21000\n",
      "reward: -0.00115546     \n",
      "loss:   5.20967e-05     \n",
      "\n",
      "iter:   22000\n",
      "reward: +0.01722        \n",
      "loss:   9.60819e-06     \n",
      "\n",
      "iter:   23000\n",
      "reward: +0.0188416      \n",
      "loss:   4.99376e-05     \n",
      "\n",
      "iter:   24000\n",
      "reward: +0.0107813      \n",
      "loss:   1.59758e-05     \n",
      "\n",
      "iter:   25000\n",
      "reward: +0.00399215     \n",
      "loss:   1.09363e-05     \n",
      "\n",
      "iter:   26000\n",
      "reward: +0.00826022     \n",
      "loss:   9.54127e-06     \n",
      "\n",
      "iter:   27000\n",
      "reward: +0.014972       \n",
      "loss:   1.42982e-05     \n",
      "\n",
      "iter:   28000\n",
      "reward: +0.00047018     \n",
      "loss:   4.01124e-05     \n",
      "\n",
      "iter:   29000\n",
      "reward: +0.000511155    \n",
      "loss:   4.86402e-05     \n",
      "\n",
      "iter:   30000\n",
      "reward: +0.0202814      \n",
      "loss:   2.56122e-06     \n",
      "\n",
      "iter:   31000\n",
      "reward: +0.0167715      \n",
      "loss:   9.1106e-06      \n",
      "\n",
      "iter:   32000\n",
      "reward: +0.0141025      \n",
      "loss:   9.76205e-06     \n"
     ]
    }
   ],
   "source": [
    "#for _ in range(1):\n",
    "while True:\n",
    "    if available_states == []:\n",
    "        # nothing left :(\n",
    "        break\n",
    "    # fill the experience replay\n",
    "    elif len(available_states) < D:\n",
    "        # getting close to the end\n",
    "        exp_rep = np.random.permutation(available_states)\n",
    "    else:\n",
    "        exp_rep = np.random.choice(available_states, size=D, replace=False)\n",
    "\n",
    "    # the actual size of the experience replay\n",
    "    d = len(exp_rep)\n",
    "    \n",
    "    # actual state values of each portfolio\n",
    "    states = np.array([st.state for st in exp_rep])\n",
    " \n",
    "    qvalues = model.predict(states)\n",
    "\n",
    "    # max_a w/ ϵ\n",
    "    chosen_actions = trd.choose_actions(qvalues, ϵ)\n",
    "\n",
    "    for (st, a) in zip(exp_rep, trd.actions[chosen_actions]):\n",
    "        # execute the action\n",
    "        st.execute_trade(a)\n",
    "\n",
    "        # step forward to the next day\n",
    "        try:\n",
    "            st.step()\n",
    "        except StopIteration:\n",
    "            # reached end of data; no more stepping for this one\n",
    "            available_states.remove(st)\n",
    "\n",
    "    states_prime = np.array([st.state for st in exp_rep])\n",
    "    rewards = np.array([reward(st) for st in  exp_rep])\n",
    "\n",
    "    # max_a' Q(s', a')\n",
    "    # use target network \n",
    "    qvalues_prime = np.max(target.predict(states_prime), axis=1)\n",
    "\n",
    "    # the values we want (to minimize the MSE of)\n",
    "    qvalues[np.arange(0,d), chosen_actions] = rewards + γ * qvalues_prime\n",
    "    \n",
    "    # train the network\n",
    "    loss = model.train_on_batch(states, qvalues)\n",
    "    loss = np.asscalar(loss[-1])\n",
    "    \n",
    "    # allow the target to drift behind\n",
    "    track_model(target, model, τ)\n",
    "    \n",
    "    if np.isnan(loss) or (np.infty in qvalues) or (np.infty in qvalues_prime):\n",
    "        # we hit the rails . . .\n",
    "        # again\n",
    "        break\n",
    "\n",
    "    # append new value\n",
    "    # not very efficient, but this probably not the slowest step\n",
    "    train_record.loc[i,:] = [np.mean(rewards), loss]\n",
    "    \n",
    "    if (DEBUG) and (i % DEBUG_EVERY == 0):\n",
    "        print('\\niter:  {:7d}\\tloss:  {:<16g}'.format(i, loss))\n",
    "        # print('reward: {:<+16g}'.format(np.mean(rewards)))\n",
    "        # print('loss:   {:<16g}'.format(loss))\n",
    "        \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_states = [State(p, cash=cash, target_weights=starting_weights, trans_cost=trans_cost) for p in test_data]\n",
    "available_test_states = test_states[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    if available_test_states == []:\n",
    "        break\n",
    "    \n",
    "    for st in available_test_states:\n",
    "        for "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
