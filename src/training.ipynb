{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import initializations\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import trading as trd\n",
    "from trading import State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'trading' from 'C:\\\\Users\\\\hamza\\\\Documents\\\\school\\\\cs_229\\\\cs229_final_project\\\\src\\\\trading\\\\__init__.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# relaod module after modifying it\n",
    "import importlib\n",
    "importlib.reload(trd)\n",
    "importlib.reload(trd.stock_history)\n",
    "importlib.reload(trd.portfolio)\n",
    "importlib.reload(trd.benchmarks)\n",
    "importlib.reload(trd.environment)\n",
    "importlib.reload(trd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulate Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transaction cost to buy/sell a stock\n",
    "trans_cost = 0.001\n",
    "# starting cash\n",
    "cash = 1e4\n",
    "# starting portfolio allocation (%lo, %hi)\n",
    "starting_weights = (0.5, 0.5)\n",
    "# reward function (either Sharpe Ratio or last reward)\n",
    "reward = trd.sharpe_ratio_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of inputs\n",
    "n = State.num_states()\n",
    "# number of outputs\n",
    "k = trd.actions.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# size of training set\n",
    "m = 27\n",
    "# size of experience replay\n",
    "d = 6\n",
    "# alpha / learning rate\n",
    "α = 0.0001\n",
    "# discount factor\n",
    "γ = 0.9\n",
    "# ϵ-greedy parameter\n",
    "ϵ = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scale = 0.001\n",
    "def my_init(shape, name=None):\n",
    "    return initializations.normal(shape, scale=scale, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(input_dim=n, output_dim=100, init=my_init),\n",
    "    Activation('relu'),\n",
    "    Dense(output_dim=100, init=my_init),\n",
    "    Activation('relu'),\n",
    "    Dense(output_dim=100, init=my_init),\n",
    "    Activation('relu'),\n",
    "    Dense(output_dim=k)])\n",
    "\n",
    "# momentum in [0.5, 0.9, 0.95, 0.99]\n",
    "# use Adam?\n",
    "sgd = SGD(lr=α, decay=1e-6, momentum=0.05, nesterov=True)\n",
    "model.compile(loss='mean_squared_error', optimizer=sgd, metrics=['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data, test_data = trd.get_stock_pairs(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "portfolio_states = [State(p, cash=cash, target_weights=starting_weights, trans_cost=trans_cost) for p in train_data]\n",
    "# list to delete from, keep all the portfolio states in portfolio_states\n",
    "#  generates a (shallow) copy rather than copy the list's reference \n",
    "available_states = portfolio_states[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prints debugging info every so many iteration \n",
    "DEBUG = True\n",
    "DEBUG_EVERY = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iter:   50\n",
      "reward: 0.234929\n",
      "loss:         0.00834743\n",
      "\n",
      "iter:   100\n",
      "reward: 0.151324\n",
      "loss:         0.00543099\n",
      "\n",
      "iter:   150\n",
      "reward: 0.0271389\n",
      "loss:         0.00371745\n"
     ]
    }
   ],
   "source": [
    "train_record = pd.DataFrame(columns=('reward', 'loss'))\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "    if available_states == []:\n",
    "        # nothing left :(\n",
    "        break\n",
    "\n",
    "    # fill the experience replay\n",
    "    exp_rep = np.random.choice(available_states, size=d, replace=False)\n",
    "\n",
    "    # actual state values of each portfolio\n",
    "    states = np.array([st.state for st in exp_rep])\n",
    "    qvalues = model.predict(states)\n",
    "\n",
    "    # max_a w/ ϵ\n",
    "    chosen_actions = trd.choose_actions(qvalues, ϵ)\n",
    "\n",
    "    for (st, a) in zip(exp_rep, trd.actions[chosen_actions]):\n",
    "        # execute the action\n",
    "        st.execute_trade(a)\n",
    "\n",
    "        # step forward to the next day\n",
    "        try:\n",
    "            st.step()\n",
    "        except StopIteration:\n",
    "            # reached end of data; no more stepping for this one\n",
    "            available_states.remove(st)\n",
    "\n",
    "    states_prime = np.array([st.state for st in exp_rep])\n",
    "    rewards = np.array([reward(st) for st in  exp_rep])\n",
    "\n",
    "    # Q(s, a) of the choosen actions (!= max_a' Q(s, a'))\n",
    "    # qvalues_curr = np.choose(chosen_action, qvalues.T)\n",
    "\n",
    "    # max_a' Q(s', a')\n",
    "    # use target network here (if doing one)\n",
    "    qvalues_prime = model.predict(states_prime)\n",
    "    qvalues_prime = np.max(qvalues_prime, axis=1)\n",
    "\n",
    "    # the target we want (to minimize the MSE of)\n",
    "    qvalues[np.arange(0,d), chosen_actions] += rewards + γ * qvalues_prime\n",
    "    \n",
    "    loss = model.train_on_batch(states, qvalues)\n",
    "    \n",
    "    # append new value\n",
    "    # not very efficient, but this probably not the slowest step\n",
    "    train_record.loc[i,:] = [np.mean(rewards), np.asscalar(loss[-1])]\n",
    "    \n",
    "    if (DEBUG) and (i % DEBUG_EVERY == 0):\n",
    "        print('\\niter:   {:d}'.format(i))\n",
    "        print('reward: {:g}'.format(np.mean(rewards)))\n",
    "        print('loss:   {:16g}'.format(np.asscalar(loss[-1])))\n",
    "        \n",
    "    i += 1"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
