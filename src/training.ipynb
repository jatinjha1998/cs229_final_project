{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras import initializations\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import trading as trd\n",
    "from trading import State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'trading' from 'C:\\\\Users\\\\hamza\\\\Documents\\\\school\\\\cs_229\\\\cs229_final_project\\\\src\\\\trading\\\\__init__.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# relaod module after modifying it\n",
    "import importlib\n",
    "importlib.reload(trd)\n",
    "importlib.reload(trd.stock_history)\n",
    "importlib.reload(trd.portfolio)\n",
    "importlib.reload(trd.benchmarks)\n",
    "importlib.reload(trd.environment)\n",
    "importlib.reload(trd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulate Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of inputs\n",
    "n = State.num_states()\n",
    "# number of outputs\n",
    "k = trd.actions.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# size of training set\n",
    "m = 27\n",
    "# size of experience replay\n",
    "d = 3\n",
    "# alpha / learning rate\n",
    "α = 0.0001\n",
    "# discount factor\n",
    "γ = 0.95\n",
    "# ϵ-greedy parameter\n",
    "ϵ = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transaction cost to buy/sell a stock\n",
    "trans_cost = 0.001\n",
    "# starting cash\n",
    "cash = 1e4\n",
    "# starting portfolio allocation (%lo, %hi)\n",
    "starting_weights = (0.5, 0.5)\n",
    "# reward function (either Sharpe Ratio or last reward)\n",
    "reward = trd.sharpe_ratio_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prints debugging info every so many iteration \n",
    "DEBUG = True\n",
    "DEBUG_EVERY = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scale = 0.001\n",
    "def my_init(shape, name=None):\n",
    "    return initializations.normal(shape, scale=scale, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(input_dim=n, output_dim=100, init=my_init),\n",
    "    Activation('relu'),\n",
    "    Dense(output_dim=100, init=my_init),\n",
    "    Activation('relu'),\n",
    "    Dense(output_dim=k)])\n",
    "\n",
    "# momentum in [0.5, 0.9, 0.95, 0.99]\n",
    "# use Adam?\n",
    "sgd = SGD(lr=α, decay=1e-6, momentum=0.05, nesterov=True)\n",
    "model.compile(loss='mean_squared_error', optimizer=sgd, metrics=['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data, test_data = trd.get_stock_pairs(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "portfolio_states = [State(p, cash=cash, target_weights=starting_weights, trans_cost=trans_cost) for p in train_data]\n",
    "# list to delete from, keep all the portfolio states in portfolio_states\n",
    "#  generates a (shallow) copy rather than copy the list's reference \n",
    "available_states = portfolio_states[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iter:   1\n",
      "reward: -0.120583\n",
      "loss:         0.00922819\n",
      "[[-0.08477008  0.06704208 -0.05822471 -0.06357161 -0.09334806 -0.08105459\n",
      "   0.0821226 ]\n",
      " [-0.08923924  0.06977071 -0.06013535  0.49448817 -0.09852659 -0.08430305\n",
      "   0.08611676]\n",
      " [-0.08414466  0.06588989 -0.05724438 -0.03698598 -0.09208937 -0.08000246\n",
      "   0.08079929]]\n",
      "\n",
      "iter:   2\n",
      "reward: 0.224866\n",
      "loss:          0.0217754\n",
      "[[-0.09000338  0.07090481 -0.06195276  0.58599728 -0.09747821 -0.0856939\n",
      "   0.08629722]\n",
      " [-0.08679777  0.06867784 -0.05973517  0.24999789 -0.09482667 -0.08294706\n",
      "   0.08371108]\n",
      " [-0.08966844  0.07037794 -0.06072672  0.62809672 -0.09848593 -0.08497159\n",
      "   0.08638484]]\n",
      "\n",
      "iter:   3\n",
      "reward: 0.102966\n",
      "loss:          0.0248774\n",
      "[[-0.11334131  0.09088433 -0.08457876  0.23713785 -0.08334183 -0.10827741\n",
      "   0.68073891]\n",
      " [-0.11233083  0.09248442 -0.088455    0.6591182  -0.07891841 -0.11038841\n",
      "   0.09043629]\n",
      " [-0.10911554  0.08625171 -0.07891845  0.1929775  -0.08119555 -0.1024804\n",
      "   0.08760024]]\n",
      "\n",
      "iter:   4\n",
      "reward: 0.117645\n",
      "loss:          0.0299944\n",
      "[[-0.12393678  0.09216308 -0.10590262  0.28052422 -0.09130508  0.58440103\n",
      "   0.16021708]\n",
      " [-0.1201168   0.09129921 -0.10854489  0.63507857 -0.08101048 -0.12003599\n",
      "   0.15583043]\n",
      " [-0.11928608  0.08866274 -0.10067836  0.3335472  -0.08527829 -0.1156623\n",
      "   0.1529596 ]]\n",
      "\n",
      "iter:   5\n",
      "reward: -0.211781\n",
      "loss:         0.00334475\n",
      "[[-0.1363796   0.08509439 -0.1063412   0.15606141 -0.07063765 -0.04702586\n",
      "   0.14036482]\n",
      " [-0.14349952  0.08993205 -0.01038948  0.29344617 -0.07488457 -0.04966592\n",
      "   0.14806001]\n",
      " [-0.14436696  0.09047791 -0.11275773  0.50716994 -0.07536713 -0.05002241\n",
      "   0.1490411 ]]\n",
      "\n",
      "iter:   6\n",
      "reward: 0.0628009\n",
      "loss:          0.0203334\n",
      "[[-0.15142135  0.48110155 -0.10997955  0.3120199  -0.07998525 -0.05300369\n",
      "   0.15178173]\n",
      " [-0.15239231  0.09589313 -0.10930262  0.80730878 -0.08188499 -0.05328561\n",
      "   0.15252714]\n",
      " [-0.14692087  0.09240604 -0.10344411  0.4795271  -0.08054697 -0.0512132\n",
      "   0.14689971]]\n",
      "\n",
      "iter:   7\n",
      "reward: 0.0949556\n",
      "loss:          0.0339306\n",
      "[[-0.1773058   0.14698755 -0.12088682  0.6777539  -0.06938487 -0.06958129\n",
      "   0.14773716]\n",
      " [-0.1865652   0.15311256 -0.12598772  0.94400159 -0.07227939 -0.07240097\n",
      "   0.15395977]\n",
      " [-0.18388513  0.15262823 -0.12411448  0.39604408 -0.0735095   0.50366834\n",
      "   0.15324009]]\n",
      "\n",
      "iter:   8\n",
      "reward: 0.0252667\n",
      "loss:          0.0397798\n",
      "[[-0.21350886  0.16455721 -0.13259891  0.72300512 -0.0595569  -0.02861092\n",
      "   0.14800165]\n",
      " [-0.2169851   0.16586869 -0.13434084  0.75173638 -0.05939879 -0.02952684\n",
      "   0.15064339]\n",
      " [-0.22798934  0.17616985 -0.14598003  1.30074682 -0.05931146 -0.03294287\n",
      "   0.159977  ]]\n",
      "\n",
      "iter:   9\n",
      "reward: 0.0373373\n",
      "loss:          0.0663569\n",
      "[[-0.26262609  0.21690751 -0.17115566  1.08557784 -0.0516135  -0.07004432\n",
      "   0.16886992]\n",
      " [-0.25875666  0.21524228 -0.16904102  1.05155128 -0.05166363 -0.06941155\n",
      "   0.1664974 ]\n",
      " [-0.27593142  0.22991208  0.80471199  0.65349486 -0.05231894 -0.07526535\n",
      "   0.17879164]]\n"
     ]
    }
   ],
   "source": [
    "train_record = pd.DataFrame(columns=('reward', 'loss'))\n",
    "loss=[]\n",
    "i = 1\n",
    "\n",
    "for _ in range(1, 50):\n",
    "    #while True:\n",
    "    # fill the experience replay\n",
    "    exp_rep = np.random.choice(available_states, size=d, replace=False)\n",
    "\n",
    "    # actual state values of each portfolio\n",
    "    states = np.array([st.state for st in exp_rep])\n",
    "    qvalues = model.predict(states)\n",
    "\n",
    "    # max_a w/ ϵ\n",
    "    chosen_actions = trd.choose_actions(qvalues, ϵ)\n",
    "\n",
    "    for (st, a) in zip(exp_rep, trd.actions[chosen_actions]):\n",
    "        # execute the action\n",
    "        st.execute_trade(a)\n",
    "\n",
    "        # step forward to the next day\n",
    "        try:\n",
    "            st.step()\n",
    "        except StopIteration:\n",
    "            # reached end of data; no more stepping for this one\n",
    "            available_states.remove(st)\n",
    "\n",
    "    states_prime = np.array([st.state for st in exp_rep])\n",
    "    rewards = np.array([reward(st) for st in  exp_rep])\n",
    "\n",
    "    # Q(s, a) of the choosen actions (!= max_a' Q(s, a'))\n",
    "    # qvalues_curr = np.choose(chosen_action, qvalues.T)\n",
    "\n",
    "    # max_a' Q(s', a')\n",
    "    # use target network here (if doing one)\n",
    "    qvalues_prime = model.predict(states_prime)\n",
    "    qvalues_prime = np.max(qvalues_prime, axis=1)\n",
    "\n",
    "    # the target we want (to minimize the MSE of)\n",
    "    qvalues[np.arange(0,d), chosen_actions] += rewards + γ * qvalues_prime\n",
    "    \n",
    "    loss += model.train_on_batch(states, qvalues)\n",
    "    \n",
    "    # append new value\n",
    "    # not very efficient, but this probably not the slowest step\n",
    "    train_record.loc[i,:] = [1, 2]\n",
    "    \n",
    "    if (DEBUG) and (i % DEBUG_EVERY == 0):\n",
    "        print('\\niter:   {:d}'.format(i))\n",
    "        print('reward: {:g}'.format(np.mean(rewards)))\n",
    "        print('loss:   {:16g}'.format(np.asscalar(loss[-1])))\n",
    "        \n",
    "    i += 1"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
